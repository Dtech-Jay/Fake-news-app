# -*- coding: utf-8 -*-
"""NEW <1>.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AOIQSm4N1T8SwJF35wxnt8l-tBYlojkq
"""



from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Replace 'path/to/your/dataset.csv' with the actual path to your CSV file in Google Drive
dataset_path = '/content/drive/MyDrive/test.csv'
df = pd.read_csv(dataset_path)

display(df.head())



"""# Task
Perform data cleaning and preprocessing on the dataset from "/content/drive/MyDrive/test.csv" to prepare it for machine learning model training, which includes handling missing values, preprocessing text columns ('statement', 'subject', 'speaker', 'speaker_description', 'context', 'justification'), encoding categorical features, engineering features from the 'date' column, and scaling numerical features.

## Initial Data Inspection

### Subtask:
Examine the DataFrame for data types, missing values, and unique values in each column to understand the cleaning requirements.

**Reasoning**:
To understand the data types, non-null values, and overall structure of the DataFrame, I will display a concise summary using the `.info()` method.
"""

print(df.info())
print("\nMissing values per column:")
print(df.isnull().sum())

text_cols = ['statement', 'subject', 'speaker', 'speaker_description', 'context', 'justification', 'label', 'date']
print("\nNumber of unique values for specified columns:")
for col in text_cols:
    if col in df.columns:
        print(f"{col}: {df[col].nunique()} unique values")
    else:
        print(f"Column '{col}' not found in DataFrame.")

print("\nDescriptive statistics for numerical columns:")
print(df.describe())

"""## Handle Missing Values

### Subtask:
Address any missing values in the dataset. This may involve imputation for numerical columns or specific strategies for categorical/text columns.

## Text Data Preprocessing

### Subtask:
Clean and preprocess the text-based column ('statement'). This will involve converting to lowercase, removing punctuation, tokenization, removing stop words, and potentially stemming or lemmatization.

**Reasoning**:
Before performing text preprocessing steps like tokenization, stop word removal, and lemmatization, it's essential to install the Natural Language Toolkit (NLTK) library and download its necessary data packages (punkt, stopwords, wordnet). This ensures all required resources are available for the subsequent text cleaning operations.
"""

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

print("NLTK and required data packages downloaded.")

"""**Reasoning**:
After downloading the necessary NLTK data, the next step is to implement the text preprocessing pipeline as outlined in the subtask instructions. This involves converting text to lowercase, removing punctuation, tokenizing, removing stop words, and lemmatizing the 'statement' column.


"""

import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# Initialize lemmatizer and load stopwords
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # 1. Convert to lowercase
    text = text.lower()
    # 2. Remove punctuation
    text = re.sub(r'[^a-z ]', '', text) # Keep only lowercase letters and spaces
    # 3. Tokenize
    tokens = word_tokenize(text)
    # 4. Remove stop words and lemmatize
    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    # 5. Join back into a string
    return ' '.join(processed_tokens)

# Apply the preprocessing function to the 'statement' column
df['statement'] = df['statement'].apply(preprocess_text)

print("Text preprocessing completed for 'statement' column. Displaying first 5 preprocessed statements:")
display(df[['statement']].head())

"""**Reasoning**:
The previous code failed because the 'punkt_tab' resource, required by NLTK's tokenizer, was not downloaded. I need to explicitly download 'punkt_tab' along with the other NLTK resources to resolve this `LookupError`.


"""

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt_tab')

print("NLTK and required data packages downloaded.")

"""**Reasoning**:
Since the `punkt_tab` resource has now been successfully downloaded, I will re-execute the text preprocessing code to apply all steps (lowercase conversion, punctuation removal, tokenization, stop word removal, and lemmatization) to the 'statement' column.


"""

import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# Initialize lemmatizer and load stopwords
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # 1. Convert to lowercase
    text = text.lower()
    # 2. Remove punctuation
    text = re.sub(r'[^a-z ]', '', text) # Keep only lowercase letters and spaces
    # 3. Tokenize
    tokens = word_tokenize(text)
    # 4. Remove stop words and lemmatize
    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    # 5. Join back into a string
    return ' '.join(processed_tokens)

# Apply the preprocessing function to the 'statement' column
df['statement'] = df['statement'].apply(preprocess_text)

print("Text preprocessing completed for 'statement' column. Displaying first 5 preprocessed statements:")
display(df[['statement']].head())

"""## Handle Categorical Features

### Subtask:
Identify any remaining categorical features in the DataFrame (other than the target 'label') and apply appropriate encoding techniques if necessary.

## Date Feature Engineering

### Subtask:
Convert the 'date' column to datetime objects and extract relevant features such as year, month, or day of the week, if applicable.

**Reasoning**:
The subtask instructs to confirm the absence of the 'date' column. I will check for the column's existence and print a message confirming its absence to fulfill this instruction.
"""

if 'date' not in df.columns:
    print("As confirmed during initial inspection, the 'date' column is not present in the DataFrame. No date feature engineering is required.")
else:
    print("The 'date' column is present in the DataFrame.")

"""## Feature Scaling (Numerical Columns)

### Subtask:
Apply scaling (e.g., StandardScaler or MinMaxScaler) to numerical features if they are to be used in models sensitive to feature scales.

### Confirmation of Numerical Features for Scaling

Based on the initial data inspection using `df.info()` and `df.describe()`, the dataset `df` contains only two columns: 'statement' (object type) and 'label' (int64 type). Since 'label' is the target variable and 'statement' is a text-based feature, there are no other numerical features present in the DataFrame that require scaling. Therefore, no feature scaling will be applied in this step.

## Final Task

### Subtask:
Summarize the data cleaning and preprocessing steps performed and confirm the data is ready for machine learning model training.

## Summary:

### Q&A
The data cleaning and preprocessing steps performed included an initial inspection, handling missing values, text preprocessing for the 'statement' column, and confirming the absence of other categorical, date, or numerical features that would require further processing. The data is now clean and ready for machine learning model training, with the 'statement' column having undergone extensive text preprocessing and the 'label' column serving as the numerical target. The next step would be to convert the preprocessed text into numerical features suitable for model input.

### Data Analysis Key Findings
*   The dataset contains 2296 entries and only two columns: 'statement' (object/text type) and 'label' (int64 type).
*   Neither the 'statement' nor the 'label' column had any missing values.
*   The 'statement' column was thoroughly preprocessed, involving conversion to lowercase, removal of punctuation, tokenization, removal of English stop words, and lemmatization.
*   The 'label' column contains 6 unique integer values, ranging from 0 to 5, indicating a multi-class classification problem.
*   Several columns mentioned in the original task description—'subject', 'speaker', 'speaker\_description', 'context', 'justification', and 'date'—were not present in the provided dataset.
*   Due to the absence of these columns and any other numerical features (besides the target 'label'), no categorical feature encoding, date feature engineering, or numerical feature scaling was performed.

### Insights or Next Steps
*   The data is now primarily a text classification problem, as the preprocessing focused exclusively on the 'statement' text and its associated numerical 'label'.
*   The next critical step is to convert the preprocessed text in the 'statement' column into numerical feature vectors (e.g., using TF-IDF, Word Embeddings, or other text vectorization techniques) before training any machine learning model.

# Task
Vectorize the preprocessed 'statement' column using TF-IDF (Term Frequency-Inverse Document Frequency) to create numerical features, and then train a classification model on these features and the 'label' target. After training, save the model and the TF-IDF vectorizer, and then create a Streamlit application that loads these saved components to allow users to input text, preprocess it, and get predictions. Finally, summarize the entire process and provide instructions on how to run the Streamlit app.

## Text Feature Vectorization

### Subtask:
Convert the preprocessed text in the 'statement' column into numerical feature vectors using a technique like TF-IDF (Term Frequency-Inverse Document Frequency) to prepare it for machine learning model training.

**Reasoning**:
To convert the preprocessed text data into numerical feature vectors using TF-IDF, I need to import `TfidfVectorizer` from `sklearn.feature_extraction.text`, initialize it, and then apply it to the 'statement' column of the DataFrame. Finally, I will print the shape of the resulting feature matrix to confirm its dimensions.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer()

# Fit and transform the 'statement' column to create TF-IDF features
X = tfidf_vectorizer.fit_transform(df['statement'])

print("Shape of the TF-IDF feature matrix (X):")
print(X.shape)

"""## Train Machine Learning Model

### Subtask:
Train a classification model (e.g., Multinomial Naive Bayes, Logistic Regression, or a simple ensemble model) on the vectorized 'statement' features and the 'label' target. Split the data into training and testing sets, train the model, and evaluate its performance.

**Reasoning**:
To train a classification model, I will start by importing the necessary libraries for data splitting, model selection (Logistic Regression), and evaluation metrics. Then, I will prepare the target variable and split the preprocessed TF-IDF features and the target into training and testing sets, ensuring reproducibility.
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

# Define target variable
y = df['label']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Data split into training and testing sets:")
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

"""**Reasoning**:
The data has been split into training and testing sets. Now, I will initialize a Logistic Regression model, train it on the training data, make predictions on the test data, and then evaluate its performance using a classification report and accuracy score, as per the subtask instructions.


"""

import warnings
warnings.filterwarnings('ignore') # Suppress convergence warning for LogisticRegression

# Initialize LogisticRegression model
model = LogisticRegression(max_iter=1000, solver='liblinear')

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)

# Evaluate the model's performance
print("Classification Report:")
print(classification_report(y_test, y_pred))
print(f"\nAccuracy Score: {accuracy_score(y_test, y_pred)}")

"""## Save Model and Vectorizer

### Subtask:
Save the trained machine learning model and the TF-IDF vectorizer using `joblib` or `pickle` so they can be loaded and used later in the Streamlit application without retraining.

**Reasoning**:
To save the trained model and TF-IDF vectorizer as instructed, I will import the `joblib` library and use its `dump` function to serialize both objects to `.joblib` files, then print a confirmation.
"""

import joblib

# Save the trained model
joblib.dump(model, 'logistic_regression_model.joblib')

# Save the TF-IDF vectorizer
joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.joblib')

print("Trained model and TF-IDF vectorizer saved successfully.")

"""## Create Streamlit Application

### Subtask:
Develop a Python script for a Streamlit application. This app will load the saved model and vectorizer, provide an input field for users to enter text, preprocess the input text, make predictions using the loaded model, and display the predicted label.

**Reasoning**:
To develop the Streamlit application as per the instructions, I need to create a Python script (`app.py`) that includes all necessary imports, NLTK data downloads, the `preprocess_text` function, loads the trained model and TF-IDF vectorizer, and sets up the Streamlit interface for user input and prediction display.
"""

import streamlit as st
import joblib
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# Download necessary NLTK data (only if not already downloaded)
# These lines are crucial for Streamlit deployment as NLTK data might not be present
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)

# Initialize lemmatizer and load stopwords
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # 1. Convert to lowercase
    text = text.lower()
    # 2. Remove punctuation
    text = re.sub(r'[^a-z ]', '', text) # Keep only lowercase letters and spaces
    # 3. Tokenize
    tokens = word_tokenize(text)
    # 4. Remove stop words and lemmatize
    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    # 5. Join back into a string
    return ' '.join(processed_tokens)

# Load the trained model and TF-IDF vectorizer
# Ensure these files are in the same directory as app.py or provide full paths
try:
    model = joblib.load('logistic_regression_model.joblib')
    tfidf_vectorizer = joblib.load('tfidf_vectorizer.joblib')
    st.success("Model and vectorizer loaded successfully!")
except FileNotFoundError:
    st.error("Error: Model or vectorizer file not found. Make sure 'logistic_regression_model.joblib' and 'tfidf_vectorizer.joblib' are in the correct directory.")
    st.stop() # Stop the app if files are not found

# Streamlit application layout
st.title("Statement Classification App")
st.write("Enter a statement below to classify its label.")

# Input field for user statement
user_statement = st.text_input("Your statement:", "")

if user_statement:
    st.write("Processing your statement...")
    # Preprocess the user's statement
    preprocessed_statement = preprocess_text(user_statement)
    st.write(f"Preprocessed statement: {preprocessed_statement}")

    # Transform the preprocessed statement using the loaded TF-IDF vectorizer
    statement_vectorized = tfidf_vectorizer.transform([preprocessed_statement])

    # Make a prediction using the loaded model
    prediction = model.predict(statement_vectorized)

    # Display the prediction
    st.subheader("Prediction:")
    st.write(f"The predicted label for your statement is: {prediction[0]}")

"""**Reasoning**:
The previous cell failed because the `streamlit` library was not installed. I need to install `streamlit` using `!pip install` before trying to import it in the Streamlit application script.


"""

import sys
!{sys.executable} -m pip install streamlit
print("Streamlit installed.")

"""**Reasoning**:
Now that Streamlit is installed, I will regenerate the Python script for the Streamlit application (`app.py`), ensuring all necessary imports, NLTK data downloads, preprocessing logic, model and vectorizer loading, and the interactive Streamlit UI are included. This script will then be saved as `app.py` in the Colab environment.


"""

import streamlit as st
import joblib
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# Download necessary NLTK data (only if not already downloaded)
# These lines are crucial for Streamlit deployment as NLTK data might not be present
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)

# Initialize lemmatizer and load stopwords
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # 1. Convert to lowercase
    text = text.lower()
    # 2. Remove punctuation
    text = re.sub(r'[^a-z ]', '', text) # Keep only lowercase letters and spaces
    # 3. Tokenize
    tokens = word_tokenize(text)
    # 4. Remove stop words and lemmatize
    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    # 5. Join back into a string
    return ' '.join(processed_tokens)

# Load the trained model and TF-IDF vectorizer
# Ensure these files are in the same directory as app.py or provide full paths
try:
    model = joblib.load('logistic_regression_model.joblib')
    tfidf_vectorizer = joblib.load('tfidf_vectorizer.joblib')
    st.success("Model and vectorizer loaded successfully!")
except FileNotFoundError:
    st.error("Error: Model or vectorizer file not found. Make sure 'logistic_regression_model.joblib' and 'tfidf_vectorizer.joblib' are in the correct directory.")
    st.stop() # Stop the app if files are not found

# Streamlit application layout
st.title("Statement Classification App")
st.write("Enter a statement below to classify its label.")

# Input field for user statement
user_statement = st.text_input("Your statement:", "")

if user_statement:
    st.write("Processing your statement...")
    # Preprocess the user's statement
    preprocessed_statement = preprocess_text(user_statement)
    st.write(f"Preprocessed statement: {preprocessed_statement}")

    # Transform the preprocessed statement using the loaded TF-IDF vectorizer
    statement_vectorized = tfidf_vectorizer.transform([preprocessed_statement])

    # Make a prediction using the loaded model
    prediction = model.predict(statement_vectorized)

    # Display the prediction
    st.subheader("Prediction:")
    st.write(f"The predicted label for your statement is: {prediction[0]}")

"""**Reasoning**:
The previous attempt to run the Streamlit application directly in a code cell resulted in warnings because Streamlit apps are designed to be run as separate scripts. To fulfill the subtask's instruction of saving the script as `app.py`, I will use the `%%writefile` magic command to create the file, rather than executing the script within the notebook cell.


"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import joblib
# import re
# import nltk
# from nltk.corpus import stopwords
# from nltk.stem import WordNetLemmatizer
# from nltk.tokenize import word_tokenize
# 
# # Download necessary NLTK data (only if not already downloaded)
# # These lines are crucial for Streamlit deployment as NLTK data might not be present
# nltk.download('punkt', quiet=True)
# nltk.download('stopwords', quiet=True)
# nltk.download('wordnet', quiet=True)
# 
# # Initialize lemmatizer and load stopwords
# lemmatizer = WordNetLemmatizer()
# stop_words = set(stopwords.words('english'))
# 
# def preprocess_text(text):
#     # 1. Convert to lowercase
#     text = text.lower()
#     # 2. Remove punctuation
#     text = re.sub(r'[^a-z ]', '', text) # Keep only lowercase letters and spaces
#     # 3. Tokenize
#     tokens = word_tokenize(text)
#     # 4. Remove stop words and lemmatize
#     processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
#     # 5. Join back into a string
#     return ' '.join(processed_tokens)
# 
# # Load the trained model and TF-IDF vectorizer
# # Ensure these files are in the same directory as app.py or provide full paths
# try:
#     model = joblib.load('logistic_regression_model.joblib')
#     tfidf_vectorizer = joblib.load('tfidf_vectorizer.joblib')
#     st.success("Model and vectorizer loaded successfully!")
# except FileNotFoundError:
#     st.error("Error: Model or vectorizer file not found. Make sure 'logistic_regression_model.joblib' and 'tfidf_vectorizer.joblib' are in the correct directory.")
#     st.stop() # Stop the app if files are not found
# 
# # Streamlit application layout
# st.title("Statement Classification App")
# st.write("Enter a statement below to classify its label.")
# 
# # Input field for user statement
# user_statement = st.text_input("Your statement:", "")
# 
# if user_statement:
#     st.write("Processing your statement...")
#     # Preprocess the user's statement
#     preprocessed_statement = preprocess_text(user_statement)
#     st.write(f"Preprocessed statement: {preprocessed_statement}")
# 
#     # Transform the preprocessed statement using the loaded TF-IDF vectorizer
#     statement_vectorized = tfidf_vectorizer.transform([preprocessed_statement])
# 
#     # Make a prediction using the loaded model
#     prediction = model.predict(statement_vectorized)
# 
#     # Display the prediction
#     st.subheader("Prediction:")
#     st.write(f"The predicted label for your statement is: {prediction[0]}")
#

"""## Final Task Summary and Streamlit App Instructions

### Summary of the Process:
1.  **Initial Data Inspection**: Examined the dataset, confirming the presence of 'statement' (text) and 'label' (numerical) columns, and the absence of others (like 'subject', 'speaker', 'date', etc.). No missing values were found in the relevant columns.
2.  **Text Data Preprocessing**: The 'statement' column underwent a comprehensive cleaning and preprocessing pipeline:
    *   NLTK data packages ('punkt', 'stopwords', 'wordnet') were downloaded.
    *   Text was converted to lowercase.
    *   Punctuation was removed.
    *   Text was tokenized.
    *   English stop words were removed.
    *   Words were lemmatized.
3.  **Feature Vectorization**: The preprocessed 'statement' column was transformed into numerical features using `TfidfVectorizer` from `sklearn.feature_extraction.text`.
4.  **Model Training**: A Logistic Regression classifier was trained on the TF-IDF features and the 'label' target. The data was split into training and testing sets, and the model's performance was evaluated.
5.  **Model and Vectorizer Saving**: The trained Logistic Regression model and the fitted TF-IDF vectorizer were saved using `joblib` for future use in the Streamlit application.
6.  **Streamlit Application Development**: A Python script (`app.py`) was created:
    *   It imports necessary libraries, including Streamlit and NLTK.
    *   It includes the `preprocess_text` function to ensure consistency with the training phase.
    *   It loads the previously saved model and vectorizer.
    *   It provides an interactive web interface for users to input text.
    *   It preprocesses the user's input, vectorizes it, and uses the loaded model to predict the label.
    *   It displays the predicted label to the user.

### How to Run the Streamlit Application (`app.py`):

To run the Streamlit application from this Google Colab environment, follow these steps:

1.  **Ensure `app.py`, `logistic_regression_model.joblib`, and `tfidf_vectorizer.joblib` are in the same directory.** (They are all saved in `/content/` by the previous steps).
2.  **Run the Streamlit command in a new code cell:**
    ```python
    !streamlit run app.py & npx localtunnel --port 8501
    ```
    *   `!streamlit run app.py`: This command starts the Streamlit server for your application.
    *   `& npx localtunnel --port 8501`: This command uses `localtunnel` (a JavaScript tool, hence `npx`) to create a public URL for your Streamlit app, which typically runs on port `8501`. You will see a public URL printed in the output (e.g., `your_random_name.loca.lt`).

3.  **Access the application**: Click on the public URL provided by `localtunnel` to interact with your Statement Classification App in your web browser.

4.  **To stop the Streamlit app**: You might need to click the 'Interrupt execution' button in Colab (the square stop button) or restart the runtime.

## Final Task Summary and Streamlit App Instructions

### Summary of the Process:
1.  **Initial Data Inspection**: Examined the dataset, confirming the presence of 'statement' (text) and 'label' (numerical) columns, and the absence of others (like 'subject', 'speaker', 'date', etc.). No missing values were found in the relevant columns.
2.  **Text Data Preprocessing**: The 'statement' column underwent a comprehensive cleaning and preprocessing pipeline:
    *   NLTK data packages ('punkt', 'stopwords', 'wordnet') were downloaded.
    *   Text was converted to lowercase.
    *   Punctuation was removed.
    *   Text was tokenized.
    *   English stop words were removed.
    *   Words were lemmatized.
3.  **Feature Vectorization**: The preprocessed 'statement' column was transformed into numerical features using `TfidfVectorizer` from `sklearn.feature_extraction.text`.
4.  **Model Training**: A Logistic Regression classifier was trained on the TF-IDF features and the 'label' target. The data was split into training and testing sets, and the model's performance was evaluated.
5.  **Model and Vectorizer Saving**: The trained Logistic Regression model and the fitted TF-IDF vectorizer were saved using `joblib` for future use in the Streamlit application.
6.  **Streamlit Application Development**: A Python script (`app.py`) was created:
    *   It imports necessary libraries, including Streamlit and NLTK.
    *   It includes the `preprocess_text` function to ensure consistency with the training phase.
    *   It loads the previously saved model and vectorizer.
    *   It provides an interactive web interface for users to input text.
    *   It preprocesses the user's input, vectorizes it, and uses the loaded model to predict the label.
    *   It displays the predicted label to the user.

### How to Run the Streamlit Application (`app.py`):

To run the Streamlit application from this Google Colab environment, follow these steps:

1.  **Ensure `app.py`, `logistic_regression_model.joblib`, and `tfidf_vectorizer.joblib` are in the same directory.** (They are all saved in `/content/` by the previous steps).
2.  **Run the Streamlit command in a new code cell:**
    ```python
    !streamlit run app.py & npx localtunnel --port 8501
    ```
    *   `!streamlit run app.py`: This command starts the Streamlit server for your application.
    *   `& npx localtunnel --port 8501`: This command uses `localtunnel` (a JavaScript tool, hence `npx`) to create a public URL for your Streamlit app, which typically runs on port `8501`. You will see a public URL printed in the output (e.g., `your_random_name.loca.lt`).

3.  **Access the application**: Click on the public URL provided by `localtunnel` to interact with your Statement Classification App in your web browser.

4.  **To stop the Streamlit app**: You might need to click the 'Interrupt execution' button in Colab (the square stop button) or restart the runtime.

## Final Task

### Subtask:
Summarize the steps taken for model training and Streamlit deployment, confirm the successful creation of the Streamlit app, and discuss how to run it.

## Summary:

### Q&A
*   **Was the Streamlit app successfully created?** Yes, a Python script named `app.py` was successfully developed and saved, containing all the necessary components for the Streamlit application.
*   **How can the Streamlit app be run?** The Streamlit app can be run in a Google Colab environment by executing the command `!streamlit run app.py & npx localtunnel --port 8501` in a new code cell. This command starts the Streamlit server and creates a public URL via `localtunnel` for access.

### Data Analysis Key Findings
*   **TF-IDF Vectorization**: The 'statement' column was successfully vectorized using `TfidfVectorizer`, resulting in a feature matrix `X` of shape (2296, 5700), indicating 2296 samples and 5700 unique terms.
*   **Model Training and Performance**: A Logistic Regression model was trained on the TF-IDF features.
    *   The data was split into training (1836 samples) and testing (460 samples) sets.
    *   The model achieved an accuracy score of approximately 0.32 (31.96%) on the test set.
    *   The classification report revealed generally low precision, recall, and f1-scores across classes (e.g., macro average f1-score of 0.17, weighted average f1-score of 0.22), with some classes having zero metrics, indicating poor predictive performance.
*   **Model and Vectorizer Saving**: The trained Logistic Regression model and the fitted TF-IDF vectorizer were successfully saved as `logistic_regression_model.joblib` and `tfidf_vectorizer.joblib` respectively, using `joblib` for later use.
*   **Streamlit Application Development**: A Python script (`app.py`) was created, which:
    *   Loads the saved model and vectorizer.
    *   Includes a `preprocess_text` function to ensure consistent text processing.
    *   Provides a web interface for user input.
    *   Makes predictions on user-provided text and displays the predicted label.
    *   Includes error handling for missing model/vectorizer files.

### Insights or Next Steps
*   The current Logistic Regression model exhibits very poor performance (32% accuracy), suggesting that further model selection, hyperparameter tuning, or exploration of more complex models (e.g., deep learning for NLP) is necessary for a robust classification system.
*   To improve the model, consider advanced text preprocessing techniques, feature engineering, or investigating class imbalance, as some classes were not predicted at all.
"""